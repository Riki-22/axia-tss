# ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ

**Document Path**: `docs/physical_design/monitoring.md`  
**Version**: 1.0  
**Type**: ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆæ›¸  
**Last Updated**: 2025-10-19

---

## ç›®æ¬¡

- [1. ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦](#1-ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦)
- [2. CloudWatchç›£è¦–è¨­è¨ˆ](#2-cloudwatchç›£è¦–è¨­è¨ˆ)
- [3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç›£è¦–](#3-ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç›£è¦–)
- [4. ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ](#4-ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ)
- [5. ãƒ­ã‚°ç®¡ç†](#5-ãƒ­ã‚°ç®¡ç†)
- [6. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ](#6-ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ)
- [7. éšœå®³å¯¾å¿œãƒ»ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³](#7-éšœå®³å¯¾å¿œã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)

---

## 1. ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦

### 1.1 ç›£è¦–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
graph TB
    subgraph "Application Layer"
        StreamlitUI[Streamlit UI<br/>ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º]
        OrderProcessor[Order Processor<br/>å–å¼•å‡¦ç†ç›£è¦–]
        DataCollector[Data Collector<br/>ãƒ‡ãƒ¼ã‚¿åé›†ç›£è¦–]
        MT5Connection[MT5 Connection<br/>æ¥ç¶šçŠ¶æ…‹ç›£è¦–]
    end
    
    subgraph "Monitoring Infrastructure"
        subgraph "CloudWatch"
            Metrics[CloudWatch Metrics<br/>ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹]
            Logs[CloudWatch Logs<br/>ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°]
            Alarms[CloudWatch Alarms<br/>é–¾å€¤ç›£è¦–]
        end
        
        subgraph "Application Monitoring"
            HealthChecks[Health Checks<br/>æ¥ç¶šçŠ¶æ…‹ç›£è¦–]
            Performance[Performance Monitoring<br/>å¿œç­”æ™‚é–“æ¸¬å®š]
            CustomMetrics[Custom Metrics<br/>ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹]
        end
    end
    
    subgraph "Alerting"
        SNS[SNS Topics<br/>ã‚¢ãƒ©ãƒ¼ãƒˆé…ä¿¡]
        Slack[Slack Notifications<br/>å³åº§é€šçŸ¥]
        Email[Email Alerts<br/>é‡è¦é€šçŸ¥]
    end
    
    StreamlitUI --> HealthChecks
    OrderProcessor --> Metrics
    DataCollector --> Logs
    MT5Connection --> Performance
    
    HealthChecks --> CustomMetrics
    Performance --> Metrics
    CustomMetrics --> Metrics
    
    Alarms --> SNS
    SNS --> Slack
    SNS --> Email
    
    classDef app fill:#e1f5fe,color:#000
    classDef monitoring fill:#e8f5e8,color:#000
    classDef alerting fill:#fff3e0,color:#000
    
    class StreamlitUI,OrderProcessor,DataCollector,MT5Connection app
    class Metrics,Logs,Alarms,HealthChecks,Performance,CustomMetrics monitoring
    class SNS,Slack,Email alerting
```

### 1.2 ç›£è¦–ãƒ¬ãƒ™ãƒ«å®šç¾©

| ç›£è¦–ãƒ¬ãƒ™ãƒ« | å¯¾è±¡ | ç›£è¦–é–“éš” | ã‚¢ãƒ©ãƒ¼ãƒˆæ–¹æ³• | å¯¾å¿œæ™‚é–“ |
|----------|------|---------|-------------|---------|
| **Critical** | å–å¼•å®Ÿè¡Œã€Kill Switch | 1åˆ† | å³åº§ï¼ˆSlackï¼‰ | å³åº§ |
| **Important** | ãƒ‡ãƒ¼ã‚¿å–å¾—ã€MT5æ¥ç¶š | 5åˆ† | 15åˆ†ä»¥å†…ï¼ˆSlackï¼‰ | 1æ™‚é–“ |  
| **Informational** | ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ | 15åˆ† | æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆEmailï¼‰ | 24æ™‚é–“ |
| **Debug** | è©³ç´°ãƒ­ã‚°ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | ç¶™ç¶š | ãƒ­ã‚°ã®ã¿ | - |

---

## 2. CloudWatchç›£è¦–è¨­è¨ˆ

### 2.1 æ¨™æº–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

#### EC2ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```yaml
Monitored Metrics:
  CPUUtilization:
    Threshold: 80%
    Period: 5 minutes
    EvaluationPeriods: 2
    ComparisonOperator: GreaterThanThreshold
    
  StatusCheckFailed:
    Threshold: 1
    Period: 1 minute
    EvaluationPeriods: 1
    ComparisonOperator: GreaterThanOrEqualToThreshold
    
  NetworkIn/NetworkOut:
    Threshold: 100MB
    Period: 5 minutes
    EvaluationPeriods: 1
    ComparisonOperator: GreaterThanThreshold
```

#### DynamoDBãƒ¡ãƒˆãƒªã‚¯ã‚¹  
```yaml
Monitored Metrics:
  ThrottledRequests:
    Threshold: 1
    Period: 5 minutes
    EvaluationPeriods: 1
    ComparisonOperator: GreaterThanOrEqualToThreshold
    
  ConsumedReadCapacityUnits:
    Threshold: 80  # ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã§ã‚‚ç›£è¦–
    Period: 5 minutes
    Statistic: Average
    
  UserErrors:
    Threshold: 5
    Period: 5 minutes
    EvaluationPeriods: 1
    ComparisonOperator: GreaterThanThreshold
```

#### ElastiCacheãƒ¡ãƒˆãƒªã‚¯ã‚¹
```yaml
Monitored Metrics:
  CPUUtilization:
    Threshold: 80%
    Period: 5 minutes
    EvaluationPeriods: 2
    
  DatabaseMemoryUsagePercentage:
    Threshold: 80%
    Period: 5 minutes
    EvaluationPeriods: 2
    
  CacheHits/CacheMisses:
    Threshold: 50%  # ãƒ’ãƒƒãƒˆç‡50%ä»¥ä¸‹ã§ã‚¢ãƒ©ãƒ¼ãƒˆ
    Period: 15 minutes
    Statistic: Average
```

### 2.2 ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨­è¨ˆï¼ˆå®Ÿè£…äºˆå®šï¼‰

```python
# src/infrastructure/monitoring/cloudwatch_publisher.py
class CloudWatchMetricsPublisher:
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch', region_name='ap-northeast-1')
        self.namespace = 'AXIA/TradingSystem'
    
    def put_trading_metrics(self, metrics: Dict[str, float]) -> None:
        """å–å¼•é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡"""
        metric_data = []
        
        for metric_name, value in metrics.items():
            metric_data.append({
                'MetricName': metric_name,
                'Value': value,
                'Unit': self._get_metric_unit(metric_name),
                'Timestamp': datetime.now(timezone.utc)
            })
        
        self.cloudwatch.put_metric_data(
            Namespace=self.namespace,
            MetricData=metric_data
        )
    
    def _get_metric_unit(self, metric_name: str) -> str:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å˜ä½ãƒãƒƒãƒ”ãƒ³ã‚°"""
        unit_mapping = {
            'ActivePositions': 'Count',
            'OrderSuccessRate': 'Percent', 
            'DataSourceResponseTime': 'Milliseconds',
            'CacheHitRate': 'Percent',
            'DailyPnL': 'Count',  # å††å˜ä½
            'KillSwitchActivations': 'Count'
        }
        return unit_mapping.get(metric_name, 'None')

# é€ä¿¡äºˆå®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¾‹
trading_metrics = {
    'ActivePositions': 2,           # ç¾åœ¨ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒã‚¸ã‚·ãƒ§ãƒ³æ•°
    'OrderSuccessRate': 98.5,       # æ³¨æ–‡æˆåŠŸç‡ï¼ˆ%ï¼‰
    'DataSourceResponseTime': 45.2, # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å¹³å‡å¿œç­”æ™‚é–“ï¼ˆmsï¼‰
    'CacheHitRate': 87.3,           # Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ï¼ˆ%ï¼‰
    'DailyPnL': 1250.0,             # å½“æ—¥æç›Šï¼ˆå††ï¼‰
    'KillSwitchActivations': 0      # Kill Switchä½œå‹•å›æ•°
}
```

### 2.3 ãƒ­ã‚°åé›†è¨­è¨ˆï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

#### ãƒ­ã‚°ã‚°ãƒ«ãƒ¼ãƒ—æ§‹æˆ
```yaml
Log Groups:
  /aws/ec2/axia-tss/application:
    RetentionInDays: 30
    LogStreams:
      - order_processor.log      # æ³¨æ–‡å‡¦ç†ãƒ­ã‚°
      - data_collector.log       # ãƒ‡ãƒ¼ã‚¿åé›†ãƒ­ã‚°  
      - streamlit.log           # UI ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°
      - mt5_connection.log      # MT5æ¥ç¶šãƒ­ã‚°
      
  /aws/ec2/axia-tss/system:
    RetentionInDays: 7
    LogStreams:
      - windows_application.log  # Windowsã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
      - task_scheduler.log      # ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ­ã‚°
      
Filter Patterns:
  ERROR: '[ERROR]'
  WARNING: '[WARNING]' 
  TRADING: '[TRADING]'
  KILL_SWITCH: '[KILL_SWITCH]'
```

---

## 3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç›£è¦–

### 3.1 ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

**å®Ÿè£…å ´æ‰€**: `src/infrastructure/monitoring/connection_checkers.py`

```python
# ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè£…
class SystemHealthChecker:
    """çµ±åˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def check_all_services(self) -> Dict[str, Any]:
        """å…¨ã‚µãƒ¼ãƒ“ã‚¹çµ±åˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        
        results = {}
        start_time = time.time()
        
        # DynamoDBæ¥ç¶šç¢ºèª
        results['dynamodb'] = self._check_dynamodb()
        
        # Redisæ¥ç¶šç¢ºèª  
        results['redis'] = self._check_redis()
        
        # MT5æ¥ç¶šç¢ºèª
        results['mt5'] = self._check_mt5()
        
        # SQSæ¥ç¶šç¢ºèª
        results['sqs'] = self._check_sqs()
        
        # å…¨ä½“åˆ¤å®š
        overall_healthy = all(
            service['connected'] 
            for service in results.values()
        )
        
        return {
            'overall_status': 'healthy' if overall_healthy else 'unhealthy',
            'services': results,
            'check_duration': time.time() - start_time,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    def _check_redis(self) -> Dict[str, Any]:
        """Redisæ¥ç¶šãƒ»æ€§èƒ½ãƒã‚§ãƒƒã‚¯"""
        start_time = time.time()
        
        try:
            redis_client = container.get_redis_client()
            
            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            redis_client.ping()
            
            # æ€§èƒ½ãƒ†ã‚¹ãƒˆ
            test_key = f"healthcheck:{int(time.time())}"
            redis_client.set(test_key, "test", ex=60)
            value = redis_client.get(test_key)
            redis_client.delete(test_key)
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—
            info = redis_client.info('memory')
            memory_used_mb = info['used_memory'] / (1024 * 1024)
            
            response_time = (time.time() - start_time) * 1000
            
            return {
                'connected': True,
                'response_time_ms': response_time,
                'memory_used_mb': round(memory_used_mb, 2),
                'memory_status': 'OK' if memory_used_mb < 50 else 'WARNING'
            }
            
        except Exception as e:
            return {
                'connected': False,
                'error': str(e),
                'response_time_ms': (time.time() - start_time) * 1000
            }
```

### 3.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

```python
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def monitor_performance(metric_name: str):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                
                # CloudWatchã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡
                put_custom_metric(
                    f"{metric_name}_ExecutionTime",
                    execution_time * 1000,  # ãƒŸãƒªç§’
                    'Milliseconds'
                )
                
                # æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
                put_custom_metric(f"{metric_name}_Success", 1, 'Count')
                
                logger.info(f"{func.__name__} completed in {execution_time:.3f}s")
                return result
                
            except Exception as e:
                execution_time = time.time() - start_time
                
                # å¤±æ•—ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                put_custom_metric(f"{metric_name}_Failure", 1, 'Count')
                
                logger.error(f"{func.__name__} failed in {execution_time:.3f}s: {e}")
                raise
        
        return wrapper
    return decorator

# ä½¿ç”¨ä¾‹
@monitor_performance('OrderProcessing')
def process_order(order_data):
    """æ³¨æ–‡å‡¦ç†ï¼ˆç›£è¦–å¯¾è±¡ï¼‰"""
    pass

@monitor_performance('DataRetrieval') 
def get_market_data(symbol, timeframe):
    """ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆç›£è¦–å¯¾è±¡ï¼‰"""
    pass
```

### 3.3 ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–

```python
# src/infrastructure/monitoring/business_metrics.py
class BusinessMetricsCollector:
    """ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
    
    def collect_trading_metrics(self) -> Dict[str, float]:
        """å–å¼•é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        
        # ãƒã‚¸ã‚·ãƒ§ãƒ³æ•°å–å¾—ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
        # active_positions = len(position_repository.find_open_positions())
        active_positions = 0  # æš«å®š
        
        # å½“æ—¥æç›Šè¨ˆç®—ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
        # daily_pnl = calculate_daily_pnl()
        daily_pnl = 0.0  # æš«å®š
        
        # Kill SwitchçŠ¶æ…‹
        kill_switch_repo = container.get_kill_switch_repository()
        kill_switch_active = 1 if kill_switch_repo.is_active() else 0
        
        # Redisçµ±è¨ˆ
        redis_stats = container.get_ohlcv_cache().get_cache_stats()
        
        return {
            'ActivePositions': active_positions,
            'KillSwitchActive': kill_switch_active,  
            'CacheMemoryUsageMB': redis_stats.get('memory_used_mb', 0),
            'CacheTotalKeys': redis_stats.get('total_keys', 0),
            'DailyPnL': daily_pnl
        }
    
    def collect_system_metrics(self) -> Dict[str, float]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        
        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        health_checker = SystemHealthChecker()
        health_status = health_checker.check_all_services()
        
        # æ¥ç¶šå¯èƒ½ã‚µãƒ¼ãƒ“ã‚¹æ•°
        connected_services = sum(
            1 for service in health_status.values() 
            if service.get('connected', False)
        )
        
        # å¹³å‡å¿œç­”æ™‚é–“è¨ˆç®—
        response_times = [
            service.get('response_time_ms', 0)
            for service in health_status.values()
            if 'response_time_ms' in service
        ]
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        
        return {
            'ConnectedServices': connected_services,
            'TotalServices': len(health_status),
            'AvgResponseTime': avg_response_time,
            'SystemHealthScore': (connected_services / len(health_status)) * 100
        }
```

---

## 4. ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ

### 4.1 ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«å®šç¾©

```mermaid
graph TB
    subgraph "Alert Severity"
        Critical[Critical<br/>å–å¼•åœæ­¢ãƒ¬ãƒ™ãƒ«<br/>å³åº§å¯¾å¿œå¿…è¦]
        High[High<br/>æ©Ÿèƒ½å½±éŸ¿ã‚ã‚Š<br/>1æ™‚é–“ä»¥å†…å¯¾å¿œ]
        Medium[Medium<br/>åŠ£åŒ–çŠ¶æ…‹<br/>24æ™‚é–“ä»¥å†…å¯¾å¿œ]
        Low[Low<br/>äºˆé˜²ä¿å®ˆ<br/>é€±æ¬¡å¯¾å¿œ]
    end
    
    subgraph "Notification Channels"
        SlackCritical[Slack @channel<br/>Critical/High]
        SlackNormal[Slack é€šå¸¸<br/>Medium]
        EmailDaily[Daily Email<br/>Low]
        Dashboard[Dashboardè¡¨ç¤º<br/>All Levels]
    end
    
    Critical --> SlackCritical
    High --> SlackCritical
    Medium --> SlackNormal
    Low --> EmailDaily
    
    Critical --> Dashboard
    High --> Dashboard
    Medium --> Dashboard
    Low --> Dashboard
```

### 4.2 ã‚¢ãƒ©ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ«è¨­å®š

#### Critical ãƒ¬ãƒ™ãƒ«
```yaml
Kill Switch Activation:
  MetricName: AXIA/TradingSystem/KillSwitchActive
  Threshold: 1
  ComparisonOperator: GreaterThanOrEqualToThreshold
  EvaluationPeriods: 1
  Period: 60  # 1åˆ†
  Actions:
    - SNS: CriticalAlertsTopicArn
  AlarmDescription: "Kill Switch has been activated"

EC2 Instance Down:
  MetricName: AWS/EC2/StatusCheckFailed
  Threshold: 1  
  ComparisonOperator: GreaterThanOrEqualToThreshold
  EvaluationPeriods: 1
  Period: 60
  Actions:
    - SNS: CriticalAlertsTopicArn
  AlarmDescription: "EC2 instance has failed status check"

DynamoDB Service Errors:
  MetricName: AWS/DynamoDB/UserErrors
  Threshold: 10
  ComparisonOperator: GreaterThanThreshold
  EvaluationPeriods: 1
  Period: 300  # 5åˆ†
```

#### High ãƒ¬ãƒ™ãƒ«
```yaml
MT5 Connection Lost:
  MetricName: AXIA/TradingSystem/MT5Connected
  Threshold: 0
  ComparisonOperator: LessThanThreshold
  EvaluationPeriods: 2
  Period: 300  # 5åˆ†
  
Redis Connection Lost:
  MetricName: AXIA/TradingSystem/RedisConnected  
  Threshold: 0
  ComparisonOperator: LessThanThreshold
  EvaluationPeriods: 2
  Period: 300

SQS DLQ Messages:
  MetricName: AWS/SQS/ApproximateNumberOfMessages
  QueueName: TSS_OrderRequestQueue_DLQ
  Threshold: 1
  ComparisonOperator: GreaterThanOrEqualToThreshold
  EvaluationPeriods: 1
  Period: 300
```

### 4.3 ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥è¨­å®š

```python
# SNS + Lambda ã«ã‚ˆã‚‹ Slacké€šçŸ¥ï¼ˆå®Ÿè£…äºˆå®šï¼‰
import json
import urllib3

def lambda_handler(event, context):
    """CloudWatch Alarm â†’ Slacké€šçŸ¥"""
    
    # SNSãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è§£æ
    for record in event['Records']:
        sns_message = json.loads(record['Sns']['Message'])
        alarm_name = sns_message['AlarmName']
        new_state = sns_message['NewStateValue']
        reason = sns_message['NewStateReason']
        
        # Slacké€šçŸ¥ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        slack_message = {
            "text": f"ğŸš¨ AXIA Alert: {alarm_name}",
            "attachments": [
                {
                    "color": "danger" if new_state == "ALARM" else "good",
                    "fields": [
                        {"title": "Status", "value": new_state, "short": True},
                        {"title": "Reason", "value": reason, "short": False},
                        {"title": "Time", "value": sns_message['StateChangeTime'], "short": True}
                    ]
                }
            ]
        }
        
        # Slack Webhooké€ä¿¡
        webhook_url = get_slack_webhook_from_secrets()
        http = urllib3.PoolManager()
        
        response = http.request(
            'POST',
            webhook_url,
            body=json.dumps(slack_message),
            headers={'Content-Type': 'application/json'}
        )
        
    return {'statusCode': 200}
```

---

## 5. ãƒ­ã‚°ç®¡ç†

### 5.1 ãƒ­ã‚°è¨­è¨ˆï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

#### æ§‹é€ åŒ–ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
```python
# å®Ÿè£…æ¸ˆã¿: src/infrastructure/config/logging_config.py
{
    "timestamp": "2025-10-19T10:00:00.123Z",
    "level": "INFO",
    "component": "order_processor", 
    "function": "execute_order",
    "message": "Order executed successfully",
    "data": {
        "order_id": "ORD-20251019-001",
        "symbol": "USDJPY",
        "action": "BUY",
        "lot_size": 0.1,
        "mt5_ticket": 12345678
    },
    "performance": {
        "execution_time_ms": 250,
        "memory_usage_mb": 45.2
    },
    "correlation_id": "req-20251019-001",
    "user_agent": "AXIA-TSS/1.0"
}
```

#### ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ä½¿ã„åˆ†ã‘
```python
# å®Ÿè£…æ¸ˆã¿ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«æˆ¦ç•¥
logger = logging.getLogger(__name__)

# CRITICAL: ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ãƒ¬ãƒ™ãƒ«
logger.critical("MT5 authentication completely failed. System shutdown required.")

# ERROR: æ©Ÿèƒ½å½±éŸ¿ãƒ¬ãƒ™ãƒ«  
logger.error("Order execution failed", extra={
    'order_id': order.ticket_id,
    'symbol': order.symbol,
    'error_code': 'MT5_CONNECTION_ERROR'
})

# WARNING: æ³¨æ„ãƒ¬ãƒ™ãƒ«
logger.warning("Redis cache miss, falling back to MT5", extra={
    'symbol': 'USDJPY',
    'timeframe': 'H1',
    'fallback_source': 'mt5'
})

# INFO: é€šå¸¸æ¥­å‹™ãƒ¬ãƒ™ãƒ«
logger.info("Order processed successfully", extra={
    'order_id': order.ticket_id,
    'mt5_ticket': mt5_ticket,
    'execution_time_ms': 250
})

# DEBUG: é–‹ç™ºãƒ»è©³ç´°èª¿æŸ»ãƒ¬ãƒ™ãƒ«
logger.debug("Cache hit for market data", extra={
    'cache_key': 'ohlcv:USDJPY:H1',
    'row_count': 240,
    'response_time_ms': 15
})
```

### 5.2 ãƒ­ã‚°åˆ†æãƒ»æ¤œç´¢

```python
# CloudWatch Insights ã‚¯ã‚¨ãƒªä¾‹
queries = {
    # ã‚¨ãƒ©ãƒ¼åˆ†æ
    "error_analysis": """
    fields @timestamp, level, component, message, data.error_code
    | filter level = "ERROR"
    | stats count() by component, data.error_code
    | sort count desc
    """,
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    "performance_analysis": """
    fields @timestamp, component, function, performance.execution_time_ms
    | filter ispresent(performance.execution_time_ms)
    | stats avg(performance.execution_time_ms), max(performance.execution_time_ms) by component, function
    | sort avg desc
    """,
    
    # å–å¼•åˆ†æ
    "trading_analysis": """
    fields @timestamp, data.symbol, data.action, data.lot_size, data.mt5_ticket
    | filter component = "order_processor" and level = "INFO"
    | filter message like /Order executed successfully/
    | stats count() by data.symbol, data.action
    """
}
```

---

## 6. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ

### 6.1 CloudWatchãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆå®Ÿè£…äºˆå®šï¼‰

```json
{
  "widgets": [
    {
      "type": "metric",
      "properties": {
        "metrics": [
          ["AWS/EC2", "CPUUtilization", "InstanceId", "i-xxxxxxxxx"],
          ["AWS/EC2", "NetworkIn", "InstanceId", "i-xxxxxxxxx"],
          ["AWS/EC2", "NetworkOut", "InstanceId", "i-xxxxxxxxx"]
        ],
        "view": "timeSeries",
        "stacked": false,
        "region": "ap-northeast-1",
        "title": "EC2 System Metrics",
        "period": 300
      }
    },
    {
      "type": "metric", 
      "properties": {
        "metrics": [
          ["AXIA/TradingSystem", "ActivePositions"],
          ["AXIA/TradingSystem", "OrderSuccessRate"],
          ["AXIA/TradingSystem", "KillSwitchActivations"]
        ],
        "view": "timeSeries",
        "region": "ap-northeast-1", 
        "title": "Trading System Metrics",
        "period": 300
      }
    },
    {
      "type": "log",
      "properties": {
        "query": "SOURCE '/aws/ec2/axia-tss/application'\n| fields @timestamp, level, component, message\n| filter level = \"ERROR\"\n| sort @timestamp desc\n| limit 100",
        "region": "ap-northeast-1",
        "title": "Recent Errors",
        "view": "table"
      }
    }
  ]
}
```

### 6.2 Streamlitç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

**å®Ÿè£…å ´æ‰€**: `src/presentation/ui/streamlit/layouts/header.py`, `sidebar.py`

```python
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º
def render_system_status():
    """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤ºï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰"""
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹å–å¾—
    health_status = controller.get_system_health()
    
    # çŠ¶æ…‹è¡¨ç¤º
    if health_status.overall_status.value == 'healthy':
        st.success("âœ… ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸ç¨¼åƒä¸­")
    else:
        st.error("âŒ ã‚·ã‚¹ãƒ†ãƒ ç•°å¸¸")
    
    # å„ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹
    for service_name, connection in health_status.connections.items():
        icon = "ğŸŸ¢" if connection.connected else "ğŸ”´"
        latency = f"{connection.latency_ms:.0f}ms" if connection.latency_ms else "N/A"
        st.text(f"{icon} {service_name.title()}: {latency}")
    
    # Kill SwitchçŠ¶æ…‹
    if health_status.kill_switch['active']:
        st.error("ğŸš¨ **KILL SWITCH ACTIVE**")
        st.text(f"ç†ç”±: {health_status.kill_switch.get('reason', 'Unknown')}")
    else:
        st.success("ğŸ’¹ å–å¼•å¯èƒ½")
```

---

## 7. éšœå®³å¯¾å¿œãƒ»ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

### 7.1 éšœå®³å¯¾å¿œãƒ•ãƒ­ãƒ¼

```mermaid
sequenceDiagram
    participant Monitor as Monitoring System
    participant Alert as Alert System
    participant Admin as Administrator
    participant System as AXIA System
    
    Note over Monitor,System: éšœå®³æ¤œçŸ¥ãƒ»å¯¾å¿œãƒ•ãƒ­ãƒ¼
    
    Monitor->>Alert: é–¾å€¤è¶…éæ¤œçŸ¥
    Alert->>Alert: ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ
    
    alt Critical Alert
        Alert->>Admin: Slack @channel å³åº§é€šçŸ¥
        Admin->>System: ç·Šæ€¥å¯¾å¿œé–‹å§‹
        Admin->>System: Kill Switchç¢ºèªãƒ»æœ‰åŠ¹åŒ–
        
    else High Alert  
        Alert->>Admin: Slack é€šå¸¸é€šçŸ¥ï¼ˆ15åˆ†ä»¥å†…ï¼‰
        Admin->>System: çŠ¶æ³ç¢ºèª
        Admin->>Monitor: è¿½åŠ ç›£è¦–è¨­å®š
        
    else Medium Alert
        Alert->>Admin: Slack é€šå¸¸é€šçŸ¥
        Note over Admin: 24æ™‚é–“ä»¥å†…å¯¾å¿œ
        
    else Low Alert
        Alert->>Admin: Daily Email
        Note over Admin: é€±æ¬¡å¯¾å¿œ
    end
    
    loop å¾©æ—§ç¢ºèª
        Monitor->>System: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        System-->>Monitor: çŠ¶æ…‹ãƒ¬ãƒãƒ¼ãƒˆ
        Monitor->>Alert: å¾©æ—§ç¢ºèª
    end
    
    Alert->>Admin: å¾©æ—§å®Œäº†é€šçŸ¥
```

### 7.2 ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹é †

#### Level 1: è‡ªå‹•å¯¾å¿œ
```python
# è‡ªå‹•å¾©æ—§ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
def auto_recovery_actions(alert_type: str):
    """è‡ªå‹•å¾©æ—§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""
    
    if alert_type == 'redis_connection_failed':
        # Redisæ¥ç¶šãƒªãƒˆãƒ©ã‚¤
        for attempt in range(3):
            try:
                redis_client = container.get_redis_client()
                redis_client.ping()
                logger.info(f"Redis reconnection successful (attempt {attempt + 1})")
                return True
            except:
                time.sleep(30)
        
        # è‡ªå‹•å¾©æ—§å¤±æ•—
        logger.error("Redis auto-recovery failed, escalating to manual intervention")
        return False
    
    elif alert_type == 'high_memory_usage':
        # Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        try:
            redis_client = container.get_redis_client()
            # å¤ã„ã‚­ãƒ¼ã®ã¿å‰Šé™¤ï¼ˆå®‰å…¨æªç½®ï¼‰
            old_keys = redis_client.keys('ohlcv:*')
            for key in old_keys:
                ttl = redis_client.ttl(key)
                if ttl < 3600:  # 1æ™‚é–“ä»¥å†…ã«æœŸé™åˆ‡ã‚Œã®ã‚­ãƒ¼ã‚’å‰Šé™¤
                    redis_client.delete(key)
            
            logger.info(f"Cleared {len(old_keys)} old cache keys")
            return True
        except Exception as e:
            logger.error(f"Auto cache cleanup failed: {e}")
            return False
```

#### Level 2: æ‰‹å‹•å¯¾å¿œ
```powershell
# æ‰‹å‹•éšœå®³å¯¾å¿œæ‰‹é †æ›¸

# 1. ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
function Check-SystemStatus {
    Write-Host "=== AXIA System Status Check ===" -ForegroundColor Cyan
    
    # ã‚µãƒ¼ãƒ“ã‚¹ç¨¼åƒçŠ¶æ³
    Get-ScheduledTask -TaskName "AXIA_*" | Format-Table TaskName,State,LastRunTime
    
    # ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª
    Get-Process | Where-Object {$_.ProcessName -match "(python|terminal64)"} | Format-Table ProcessName,Id,WorkingSet
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶š
    Test-NetConnection -ComputerName localhost -Port 8501
    Test-NetConnection -ComputerName axia-redis-cache.xxxxx.cache.amazonaws.com -Port 6379
    
    # AWS ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹
    aws dynamodb describe-table --table-name TSS_DynamoDB_OrderState --query 'Table.TableStatus'
}

# 2. ç·Šæ€¥æ™‚å¯¾å¿œ
function Emergency-Response {
    Write-Host "=== EMERGENCY RESPONSE STARTED ===" -ForegroundColor Red
    
    # Kill Switchæœ‰åŠ¹åŒ–
    aws dynamodb put-item --table-name TSS_DynamoDB_OrderState --item '{
      "pk": {"S": "GLOBALCONFIG"},
      "sk": {"S": "SETTING#KILL_SWITCH"},
      "active": {"BOOL": true},
      "reason": {"S": "Emergency manual activation"}
    }'
    
    # å…¨ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
    Get-ScheduledTask -TaskName "AXIA_*" | Stop-ScheduledTask -Force
    
    # MT5å…¨ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¯ãƒ­ãƒ¼ã‚ºï¼ˆæ‰‹å‹•ç¢ºèªè¦ï¼‰
    Write-Host "âš ï¸  Manual Action Required: Close all MT5 positions manually" -ForegroundColor Yellow
}
```

### 7.3 éšœå®³åˆ†æãƒ»æ”¹å–„

```python
# éšœå®³å¾Œåˆ†æï¼ˆå®Ÿè£…äºˆå®šï¼‰
class IncidentAnalyzer:
    """éšœå®³åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def analyze_incident(self, start_time: datetime, end_time: datetime) -> Dict:
        """éšœå®³æœŸé–“ã®åˆ†æ"""
        
        # 1. ãƒ­ã‚°åˆ†æ
        error_logs = self.query_cloudwatch_logs(
            log_group='/aws/ec2/axia-tss/application',
            start_time=start_time,
            end_time=end_time,
            filter_pattern='[timestamp, level="ERROR", ...]'
        )
        
        # 2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ
        metrics_data = self.get_cloudwatch_metrics(
            namespace='AXIA/TradingSystem',
            start_time=start_time,
            end_time=end_time
        )
        
        # 3. å½±éŸ¿ç¯„å›²åˆ†æ
        impact_analysis = {
            'affected_orders': len([log for log in error_logs if 'order' in log['message']]),
            'data_collection_failures': len([log for log in error_logs if 'data_collector' in log['component']]),
            'user_impact': 'UI unavailable' if any('streamlit' in log['component'] for log in error_logs) else 'minimal'
        }
        
        return {
            'incident_duration': (end_time - start_time).total_seconds(),
            'error_count': len(error_logs),
            'primary_cause': self._identify_primary_cause(error_logs),
            'impact_analysis': impact_analysis,
            'recovery_actions': self._extract_recovery_actions(error_logs)
        }
```

---

## ä»˜éŒ²

### A. ç›£è¦–é …ç›®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

| ã‚«ãƒ†ã‚´ãƒª | ç›£è¦–é …ç›® | å®Ÿè£…çŠ¶æ³ | å„ªå…ˆåº¦ |
|---------|---------|---------|--------|
| **ã‚·ã‚¹ãƒ†ãƒ ** | EC2ç¨¼åƒçŠ¶æ³ | âœ… CloudWatch | Critical |
| **ã‚·ã‚¹ãƒ†ãƒ ** | CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ | âœ… CloudWatch | High |
| **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³** | Kill SwitchçŠ¶æ…‹ | âœ… Streamlit | Critical |
| **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³** | å–å¼•å®Ÿè¡ŒæˆåŠŸç‡ | ğŸ”„ è¨­è¨ˆæ¸ˆã¿ | Critical |
| **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹** | DynamoDBæ¥ç¶š | âœ… ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ | High |
| **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹** | Redisæ¥ç¶šãƒ»ãƒ¡ãƒ¢ãƒª | âœ… ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ | High |
| **å¤–éƒ¨æ¥ç¶š** | MT5æ¥ç¶šçŠ¶æ…‹ | âœ… ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ | Critical |
| **å¤–éƒ¨æ¥ç¶š** | yfinance API | âŒ æœªå®Ÿè£… | Medium |

### B. ã‚¢ãƒ©ãƒ¼ãƒˆé »åº¦ç®¡ç†

```python
# ã‚¢ãƒ©ãƒ¼ãƒˆæŠ‘åˆ¶æ©Ÿèƒ½ï¼ˆå®Ÿè£…äºˆå®šï¼‰
class AlertSuppressionManager:
    """ã‚¢ãƒ©ãƒ¼ãƒˆé »åº¦åˆ¶å¾¡"""
    
    def __init__(self):
        self.alert_history = {}  # {alert_type: last_sent_time}
        
    def should_send_alert(self, alert_type: str, severity: str) -> bool:
        """ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡åˆ¤å®š"""
        
        # Critical ã¯å¸¸ã«é€ä¿¡
        if severity == 'Critical':
            return True
            
        # åŒã˜ã‚¢ãƒ©ãƒ¼ãƒˆã®é€ä¿¡é–“éš”åˆ¶å¾¡
        min_intervals = {
            'High': 300,      # 5åˆ†é–“éš”
            'Medium': 3600,   # 1æ™‚é–“é–“éš”  
            'Low': 86400      # 24æ™‚é–“é–“éš”
        }
        
        min_interval = min_intervals.get(severity, 3600)
        last_sent = self.alert_history.get(alert_type, 0)
        
        if time.time() - last_sent >= min_interval:
            self.alert_history[alert_type] = time.time()
            return True
            
        return False
```

### C. ç›£è¦–ã‚³ã‚¹ãƒˆæœ€é©åŒ–

| æœ€é©åŒ–é …ç›® | ç¾åœ¨ | æœ€é©åŒ–å¾Œ | ç¯€ç´„åŠ¹æœ |
|----------|------|---------|---------|
| **CloudWatch Logs** | 30æ—¥ä¿æŒ | 7æ—¥ä¿æŒï¼ˆé‡è¦ãƒ­ã‚°ã®ã¿30æ—¥ï¼‰ | 50%å‰Šæ¸› |
| **CloudWatch Metrics** | å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | é‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿ | 30%å‰Šæ¸› |
| **ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹** | æœªå®Ÿè£… | å¿…è¦æœ€å°é™ | ã‚³ã‚¹ãƒˆå¢—åŠ æŠ‘åˆ¶ |
| **ã‚¢ãƒ©ãƒ¼ãƒ æ•°** | åŸºæœ¬ã®ã¿ | é‡è¦ã‚¢ãƒ©ãƒ¼ãƒ ã®ã¿ | 60%å‰Šæ¸› |

---

**Document Version**: 1.0  
**Last Updated**: 2025-10-19  
**Next Review**: 2025-11-19